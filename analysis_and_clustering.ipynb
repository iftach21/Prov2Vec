{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "tvxGaD6TH6ld",
        "hQX3JgxNH-0L",
        "vmpp2yZ_jG6-"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Our research running:**\n",
        "\n",
        "\n",
        "1.   English proverbs analysis with clustering and cosine similarity.\n",
        "2.   Chinese proverbs analysis with clustering under the pre-assigned classes.\n",
        "3.   Multy language analysis and clustering - using all of our data.\n",
        "\n",
        "\n",
        "Then we took those results and looked up for intresting finding for our paper.\n",
        "\n"
      ],
      "metadata": {
        "id": "QEv2kH_Qg0dj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "\n",
        "\n",
        "# Load all of the data names from the exel files for future use:\n",
        "\n",
        "\n",
        "# language - the file name for it.\n",
        "\n",
        "english = \"4437_English_proverbs With vectors.xlsx\"\n",
        "\n",
        "hebrew = \"updated_1867_Hebrew_Proverbs with Vectors.xlsx\"\n",
        "\n",
        "arabic = \"updated_kaggle_Arabic_proverbs with Vectors.xlsx\"\n",
        "\n",
        "french = \"updated_kaggle_French_proverbs with Vectors.xlsx\"\n",
        "\n",
        "chinese = \"updated_kaggle_Chinese_proverbs with Vectors.xlsx\"\n",
        "\n",
        "#The Excel vector columns of the embedding for each language.\n",
        "vector_columns = [\n",
        "    \"Vector bert-base-uncased\",\n",
        "    \"Vector paraphrase-multilingual-MiniLM-L12-v2\",\n",
        "    \"Vector All-MiniLM-L12-v2\",\n",
        "    \"Vector roberta-large-nli-stsb-mean-tokens\"\n",
        "]"
      ],
      "metadata": {
        "id": "ES6LqDmwC0UW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering and analysing using the English proverbs"
      ],
      "metadata": {
        "id": "tvxGaD6TH6ld"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6zYaSvKyGHa",
        "outputId": "246bfebb-84ca-44b7-be31-d1acac3a13ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing for vector representation: Vector bert-base-uncased\n",
            "Similar pairs for Vector bert-base-uncased:\n",
            "                                              sentence_1  \\\n",
            "58424                    englishman's home is his castle   \n",
            "38761                                          acid test   \n",
            "38760                                          acid test   \n",
            "31170                                          in spades   \n",
            "76525                               fight the good fight   \n",
            "...                                                  ...   \n",
            "76404  famous last words (dying statements of famous ...   \n",
            "17480                                      alphabet soup   \n",
            "18652                                   blow a raspberry   \n",
            "21577                                       gregory peck   \n",
            "74817             all promises are either broken or kept   \n",
            "\n",
            "                                              sentence_2  similarity  cluster  \n",
            "58424                                     zero tolerance    0.910000       11  \n",
            "38761                          if the shoe fits, wear it    0.909999       10  \n",
            "38760                          if the shoe fits, wear it    0.909999       10  \n",
            "31170                                      wine and dine    0.909999       19  \n",
            "76525  keep a thing seven years and you will find a u...    0.909999        8  \n",
            "...                                                  ...         ...      ...  \n",
            "76404    it's one thing to flourish and another to fight    0.751796        8  \n",
            "17480                                     swing the lead    0.751133       15  \n",
            "18652                                       gregory peck    0.750718       15  \n",
            "21577                                            jam jar    0.750453       15  \n",
            "74817  famous last words (dying statements of famous ...    0.750336        8  \n",
            "\n",
            "[78949 rows x 4 columns]\n",
            "Saved similar pairs to similar_pairs_Vector_bert-base-uncased.csv\n",
            "Processing for vector representation: Vector paraphrase-multilingual-MiniLM-L12-v2\n",
            "Similar pairs for Vector paraphrase-multilingual-MiniLM-L12-v2:\n",
            "                                 sentence_1  \\\n",
            "866      better to do well than to say well   \n",
            "12116            you can't take it with you   \n",
            "12117            you can't take it with you   \n",
            "12980                 it takes two to tango   \n",
            "12979                 it takes two to tango   \n",
            "...                                     ...   \n",
            "25541              beauty is only skin deep   \n",
            "24406  no use crying over spilt milk – it’s   \n",
            "18820                              crackpot   \n",
            "23209             exception proves the rule   \n",
            "19981     all things come to those who wait   \n",
            "\n",
            "                                      sentence_2  similarity  cluster  \n",
            "866                  doing is better than saying    0.909961       16  \n",
            "12116  you can’t take it with you [when you die]    0.909895        8  \n",
            "12117  you can’t take it with you [when you die]    0.909895        8  \n",
            "12980                           tell me about it    0.909893        3  \n",
            "12979                           tell me about it    0.909893        3  \n",
            "...                                          ...         ...      ...  \n",
            "25541    person is known by the company he keeps    0.750006       18  \n",
            "24406          small leak will sink a great ship    0.750006       19  \n",
            "18820            sledgehammer to crack a nut - a    0.750005        6  \n",
            "23209      it ain’t over till the fat lady sings    0.750002       19  \n",
            "19981                       hope springs eternal    0.750000       19  \n",
            "\n",
            "[27414 rows x 4 columns]\n",
            "Saved similar pairs to similar_pairs_Vector_paraphrase-multilingual-MiniLM-L12-v2.csv\n",
            "Processing for vector representation: Vector All-MiniLM-L12-v2\n",
            "Similar pairs for Vector All-MiniLM-L12-v2:\n",
            "                                            sentence_1  \\\n",
            "118                         as pure as the driven snow   \n",
            "314  female of the species is more deadly than the ...   \n",
            "83   don’t shut the stable door after the horse has...   \n",
            "152                         singer not the song – it’s   \n",
            "772                god helps those who help themselves   \n",
            "..                                                 ...   \n",
            "922         cobbler always wears the worst shoes – the   \n",
            "234  he is not poor that has little, but he that de...   \n",
            "836  better to have loved and lost than never to ha...   \n",
            "838  better to have loved and lost than never to ha...   \n",
            "514       doubt is the beginning not the end of wisdom   \n",
            "\n",
            "                                            sentence_2  similarity  cluster  \n",
            "118                            pure as the driven snow    0.909920        6  \n",
            "314  female of the species is more deadly then the ...    0.909896       10  \n",
            "83   it’s no use shutting the stable door after the...    0.909805        2  \n",
            "152                       it’s the singer not the song    0.909575        6  \n",
            "772  lord (god, heaven) helps those (them) who help...    0.909252        8  \n",
            "..                                                 ...         ...      ...  \n",
            "922               shoemaker’s son always goes barefoot    0.750281       14  \n",
            "234  poverty is not a shame, but the being ashamed ...    0.750248        5  \n",
            "836          heart that once truly loves never forgets    0.750161        1  \n",
            "838          heart that once truly loves never forgets    0.750161        1  \n",
            "514               he that knows nothing doubts nothing    0.750131       19  \n",
            "\n",
            "[1021 rows x 4 columns]\n",
            "Saved similar pairs to similar_pairs_Vector_All-MiniLM-L12-v2.csv\n",
            "Processing for vector representation: Vector roberta-large-nli-stsb-mean-tokens\n",
            "Similar pairs for Vector roberta-large-nli-stsb-mean-tokens:\n",
            "                                sentence_1  \\\n",
            "2458                    pull in your horns   \n",
            "3837              bygones be bygones - let   \n",
            "3838              bygones be bygones - let   \n",
            "3839              bygones be bygones - let   \n",
            "178     no accounting for tastes – there’s   \n",
            "...                                    ...   \n",
            "2307          don’t meet troubles half-way   \n",
            "3439  every dark cloud has a silver lining   \n",
            "1740     ask no questions and hear no lies   \n",
            "1739     ask no questions and hear no lies   \n",
            "3309    all things come to those that wait   \n",
            "\n",
            "                                       sentence_2  similarity  cluster  \n",
            "2458                to draw (pull) in one's horns    0.909978        1  \n",
            "3837                       let bygones be bygones    0.909494       16  \n",
            "3838                       let bygones be bygones    0.909494       16  \n",
            "3839                       let bygones be bygones    0.909494       16  \n",
            "178              there’s no accounting for tastes    0.909163        6  \n",
            "...                                           ...         ...      ...  \n",
            "2307                    good counsel does no harm    0.750037        1  \n",
            "3439  it’s an ill wind that blows no one any good    0.750010       12  \n",
            "1740                            silence is golden    0.750006       10  \n",
            "1739                            silence is golden    0.750006       10  \n",
            "3309                                    bode well    0.750005       12  \n",
            "\n",
            "[4138 rows x 4 columns]\n",
            "Saved similar pairs to similar_pairs_Vector_roberta-large-nli-stsb-mean-tokens.csv\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "def load_data(file_path):\n",
        "    data = pd.read_excel(file_path)\n",
        "    return data\n",
        "\n",
        "# Convert vector column from string to numpy array\n",
        "def process_vectors(data, vector_column):\n",
        "    data[vector_column] = data[vector_column].apply(lambda x: np.array(eval(x)))\n",
        "    return data\n",
        "\n",
        "# Perform clustering\n",
        "def perform_clustering(vectors, num_clusters):\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "    clusters = kmeans.fit_predict(list(vectors))\n",
        "    return clusters\n",
        "\n",
        "# Find pairs with high cosine similarity within each cluster\n",
        "def find_similar_pairs(data, vector_column, cluster_column, threshold=0.75):\n",
        "    results = []\n",
        "    for cluster_id in data[cluster_column].unique():\n",
        "        cluster_data = data[data[cluster_column] == cluster_id]\n",
        "        vectors = np.stack(cluster_data[vector_column].values)\n",
        "        similarities = cosine_similarity(vectors)\n",
        "        n = len(cluster_data)\n",
        "\n",
        "        for i in range(n):\n",
        "            for j in range(i + 1, n):\n",
        "                if threshold <= similarities[i, j] <= 0.91:\n",
        "                    results.append({\n",
        "                        \"sentence_1\": cluster_data.iloc[i][\"trimmed\"],\n",
        "                        \"sentence_2\": cluster_data.iloc[j][\"trimmed\"],\n",
        "                        \"similarity\": similarities[i, j],\n",
        "                        \"cluster\": cluster_id\n",
        "                    })\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Main workflow\n",
        "def main(file_path, vector_columns, num_clusters=20, similarity_threshold=0.75, output_csv=True):\n",
        "    data = load_data(file_path)\n",
        "\n",
        "    for vector_column in vector_columns:\n",
        "        print(f\"Processing for vector representation: {vector_column}\")\n",
        "\n",
        "        # Process vectors\n",
        "        data = process_vectors(data, vector_column)\n",
        "\n",
        "        # Perform clustering\n",
        "        clusters = perform_clustering(data[vector_column], num_clusters)\n",
        "        cluster_column = f\"{vector_column}_cluster\"\n",
        "        data[cluster_column] = clusters\n",
        "\n",
        "        # Find similar pairs within clusters\n",
        "        similar_pairs = find_similar_pairs(data, vector_column, cluster_column, similarity_threshold)\n",
        "        similar_pairs = similar_pairs.sort_values(by=\"similarity\", ascending=False)\n",
        "        print(f\"Similar pairs for {vector_column}:\")\n",
        "        print(similar_pairs)\n",
        "\n",
        "        # Save results to CSV if required\n",
        "        if output_csv:\n",
        "            output_file = f\"similar_pairs_{vector_column.replace(' ', '_')}.csv\"\n",
        "            similar_pairs.to_csv(output_file, index=False)\n",
        "            print(f\"Saved similar pairs to {output_file}\")\n",
        "\n",
        "\n",
        "# Run the workflow\n",
        "main(file_path, vector_columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering the Chinese proverbs."
      ],
      "metadata": {
        "id": "hQX3JgxNH-0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code was not used to run the clustering, but is still saved."
      ],
      "metadata": {
        "id": "3b5kJrUJiFln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_path):\n",
        "    \"\"\"Load the dataset from an Excel file.\"\"\"\n",
        "    data = pd.read_excel(file_path)\n",
        "    return data\n",
        "\n",
        "def process_vectors(data, vector_column):\n",
        "    \"\"\"Convert vector column from string to numpy array.\"\"\"\n",
        "    data[vector_column] = data[vector_column].apply(lambda x: np.array(eval(x)))\n",
        "    return data\n",
        "\n",
        "def perform_clustering(vectors, num_clusters):\n",
        "    \"\"\"Perform clustering using KMeans.\"\"\"\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "    clusters = kmeans.fit_predict(list(vectors))\n",
        "    return clusters\n",
        "\n",
        "def evaluate_clustering(data, cluster_column, category_column):\n",
        "    \"\"\"Evaluate clustering using Adjusted Rand Index (ARI).\"\"\"\n",
        "    ari = adjusted_rand_score(data[category_column], data[cluster_column])\n",
        "    return ari\n",
        "\n",
        "def find_similar_pairs(data, vector_column, cluster_column, threshold=0.75):\n",
        "    \"\"\"Find pairs with high cosine similarity within each cluster.\"\"\"\n",
        "    results = []\n",
        "    for cluster_id in data[cluster_column].unique():\n",
        "        cluster_data = data[data[cluster_column] == cluster_id]\n",
        "        vectors = np.stack(cluster_data[vector_column].values)\n",
        "        similarities = cosine_similarity(vectors)\n",
        "        n = len(cluster_data)\n",
        "\n",
        "        for i in range(n):\n",
        "            for j in range(i + 1, n):\n",
        "                if threshold <= similarities[i, j] <= 0.91:\n",
        "                    results.append({\n",
        "                        \"sentence_1\": cluster_data.iloc[i][\"trimmed\"],\n",
        "                        \"sentence_2\": cluster_data.iloc[j][\"trimmed\"],\n",
        "                        \"similarity\": similarities[i, j],\n",
        "                        \"cluster\": cluster_id\n",
        "                    })\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def main(file_path, vector_columns, category_column=\"category\", similarity_threshold=0.75, output_csv=False):\n",
        "    \"\"\"Main workflow.\"\"\"\n",
        "    # Load the data\n",
        "    data = load_data(file_path)\n",
        "\n",
        "    # Determine the number of unique categories for clustering\n",
        "    num_clusters = data[category_column].nunique()\n",
        "\n",
        "    results_summary = []\n",
        "\n",
        "    for vector_column in vector_columns:\n",
        "        print(f\"Processing for vector representation: {vector_column}\")\n",
        "\n",
        "        # Process vectors\n",
        "        data = process_vectors(data, vector_column)\n",
        "\n",
        "        # Perform clustering\n",
        "        clusters = perform_clustering(data[vector_column], num_clusters)\n",
        "        cluster_column = f\"{vector_column}_cluster\"\n",
        "        data[cluster_column] = clusters\n",
        "\n",
        "        # Evaluate clustering performance\n",
        "        ari = evaluate_clustering(data, cluster_column, category_column)\n",
        "        print(f\"Adjusted Rand Index for {vector_column}: {ari}\")\n",
        "        results_summary.append({\"vector_column\": vector_column, \"ari\": ari})\n",
        "\n",
        "        # Find similar pairs within clusters\n",
        "        similar_pairs = find_similar_pairs(data, vector_column, cluster_column, similarity_threshold)\n",
        "        similar_pairs = similar_pairs.sort_values(by=\"similarity\", ascending=False)\n",
        "        print(f\"Similar pairs for {vector_column}:\\n\", similar_pairs.head())\n",
        "\n",
        "        # Save results to CSV if required\n",
        "        if output_csv:\n",
        "            output_file = f\"similar_pairs_{vector_column.replace(' ', '_')}.csv\"\n",
        "            similar_pairs.to_csv(output_file, index=False)\n",
        "            print(f\"Saved similar pairs to {output_file}\")\n",
        "\n",
        "    # Summary of ARI scores\n",
        "    results_summary_df = pd.DataFrame(results_summary)\n",
        "    print(\"\\nSummary of Adjusted Rand Index scores:\")\n",
        "    print(results_summary_df)\n",
        "    if output_csv:\n",
        "        results_summary_df.to_csv(\"clustering_evaluation_summary.csv\", index=False)\n",
        "        print(\"Saved clustering evaluation summary to clustering_evaluation_summary.csv\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# main(chin, vector_columns, category_column=\"category\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoATC1CBICZZ",
        "outputId": "a150f850-ca21-428f-e9ec-d2bd4d905000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing for vector representation: Vector bert-base-uncased\n",
            "Adjusted Rand Index for Vector bert-base-uncased: 0.14895844413218606\n",
            "Similar pairs for Vector bert-base-uncased:\n",
            "      sentence_1        sentence_2  similarity  cluster\n",
            "6      有情人终成眷属。          桂林山水甲天下。    0.909712        6\n",
            "1      一日之计在于晨。             逆来顺受。    0.909106        3\n",
            "8  但愿人长久，千里共婵娟。             龙马精神。    0.908993        6\n",
            "0     道不同，不相为谋。  一寸光阴一寸金，寸金难买寸光阴。    0.908515        3\n",
            "9  但愿人长久，千里共婵娟。             车水马龙。    0.907023        6\n",
            "Saved similar pairs to similar_pairs_Vector_bert-base-uncased.csv\n",
            "Processing for vector representation: Vector paraphrase-multilingual-MiniLM-L12-v2\n",
            "Adjusted Rand Index for Vector paraphrase-multilingual-MiniLM-L12-v2: 0.11347488852535291\n",
            "Similar pairs for Vector paraphrase-multilingual-MiniLM-L12-v2:\n",
            "        sentence_1    sentence_2  similarity  cluster\n",
            "92          知音难觅。      广交友，无深交。    0.887603        5\n",
            "128         龙飞凤舞。         龙马精神。    0.875140        6\n",
            "54       清官难断家务事。       家丑不可外扬。    0.872421        4\n",
            "129         龙飞凤舞。         龙腾虎跃。    0.867451        6\n",
            "113  愿得一人心，白首不相离。  但愿人长久，千里共婵娟。    0.839501        5\n",
            "Saved similar pairs to similar_pairs_Vector_paraphrase-multilingual-MiniLM-L12-v2.csv\n",
            "Processing for vector representation: Vector All-MiniLM-L12-v2\n",
            "Adjusted Rand Index for Vector All-MiniLM-L12-v2: 0.05656256317127957\n",
            "Similar pairs for Vector All-MiniLM-L12-v2:\n",
            "   sentence_1 sentence_2  similarity  cluster\n",
            "0      龙飞凤舞。      龙腾虎跃。    0.805100        1\n",
            "2    一步一个脚印。      一举两得。    0.800501        4\n",
            "1      龙腾虎跃。     鲤鱼跳龙门。    0.756421        1\n",
            "Saved similar pairs to similar_pairs_Vector_All-MiniLM-L12-v2.csv\n",
            "Processing for vector representation: Vector roberta-large-nli-stsb-mean-tokens\n",
            "Adjusted Rand Index for Vector roberta-large-nli-stsb-mean-tokens: 0.06427047546400723\n",
            "Similar pairs for Vector roberta-large-nli-stsb-mean-tokens:\n",
            "      sentence_1 sentence_2  similarity  cluster\n",
            "4        欲速则不达。  不能一口吃成胖子。    0.837690        4\n",
            "7  愿得一人心，白首不相离。   有情人终成眷属。    0.809752        5\n",
            "2        一笑解千愁。   笑一笑,十年少。    0.805406        6\n",
            "5  吃得苦中苦，方为人上人。     逆境出人才。    0.784076        4\n",
            "3        一笑解千愁。   人逢喜事精神爽。    0.783517        6\n",
            "Saved similar pairs to similar_pairs_Vector_roberta-large-nli-stsb-mean-tokens.csv\n",
            "\n",
            "Summary of Adjusted Rand Index scores:\n",
            "                                  vector_column       ari\n",
            "0                      Vector bert-base-uncased  0.148958\n",
            "1  Vector paraphrase-multilingual-MiniLM-L12-v2  0.113475\n",
            "2                      Vector All-MiniLM-L12-v2  0.056563\n",
            "3     Vector roberta-large-nli-stsb-mean-tokens  0.064270\n",
            "Saved clustering evaluation summary to clustering_evaluation_summary.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doing clustering on chinese proverbs"
      ],
      "metadata": {
        "id": "gHKQ_d5XalMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, homogeneity_score, completeness_score, v_measure_score, fowlkes_mallows_score\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"Load the dataset from an Excel file.\"\"\"\n",
        "    data = pd.read_excel(file_path)\n",
        "    return data\n",
        "\n",
        "def process_vectors(data, vector_column):\n",
        "    \"\"\"Convert vector column from string to numpy array.\"\"\"\n",
        "    data[vector_column] = data[vector_column].apply(lambda x: np.array(eval(x)))\n",
        "    return data\n",
        "\n",
        "def perform_clustering(vectors, num_clusters):\n",
        "    \"\"\"Perform KMeans clustering.\"\"\"\n",
        "    model = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "    return model.fit_predict(list(vectors))\n",
        "\n",
        "def evaluate_clustering(data, cluster_column, category_column, vectors):\n",
        "    \"\"\"Evaluate clustering using various metrics.\"\"\"\n",
        "    results = {}\n",
        "\n",
        "    # Supervised metrics (require ground truth labels)\n",
        "    results[\"Adjusted Rand Index\"] = adjusted_rand_score(data[category_column], data[cluster_column])\n",
        "    results[\"Normalized Mutual Information\"] = normalized_mutual_info_score(data[category_column], data[cluster_column])\n",
        "    results[\"Homogeneity\"] = homogeneity_score(data[category_column], data[cluster_column])\n",
        "    results[\"Completeness\"] = completeness_score(data[category_column], data[cluster_column])\n",
        "    results[\"V-Measure\"] = v_measure_score(data[category_column], data[cluster_column])\n",
        "    results[\"Fowlkes-Mallows Index\"] = fowlkes_mallows_score(data[category_column], data[cluster_column])\n",
        "\n",
        "    # Unsupervised metrics (no ground truth required)\n",
        "    results[\"Silhouette Score\"] = silhouette_score(list(vectors), data[cluster_column], metric='cosine')\n",
        "    results[\"Davies-Bouldin Index\"] = davies_bouldin_score(list(vectors), data[cluster_column])\n",
        "    results[\"Calinski-Harabasz Index\"] = calinski_harabasz_score(list(vectors), data[cluster_column])\n",
        "\n",
        "    return results\n",
        "\n",
        "def main(file_path, vector_columns, category_column=\"category\", output_csv=True):\n",
        "    \"\"\"Main workflow.\"\"\"\n",
        "    # Load the data\n",
        "    data = load_data(file_path)\n",
        "\n",
        "    # Determine the number of unique categories for clustering\n",
        "    num_clusters = data[category_column].nunique()\n",
        "\n",
        "    results_summary = []\n",
        "\n",
        "    for vector_column in vector_columns:\n",
        "        print(f\"Processing for vector representation: {vector_column}\")\n",
        "\n",
        "        # Process vectors\n",
        "        data = process_vectors(data, vector_column)\n",
        "\n",
        "        # Perform clustering\n",
        "        clusters = perform_clustering(data[vector_column], num_clusters=num_clusters)\n",
        "        cluster_column = f\"{vector_column}_cluster\"\n",
        "        data[cluster_column] = clusters\n",
        "\n",
        "        # Evaluate clustering performance\n",
        "        evaluation_results = evaluate_clustering(data, cluster_column, category_column, data[vector_column])\n",
        "        print(f\"Evaluation results for {vector_column}:\")\n",
        "        for metric, value in evaluation_results.items():\n",
        "            print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "        # Find the best metric and its value\n",
        "        best_metric = max(evaluation_results, key=evaluation_results.get)\n",
        "        best_value = evaluation_results[best_metric]\n",
        "        print(f\"Best Metric for {vector_column}: {best_metric} with value {best_value:.4f}\\n\")\n",
        "\n",
        "        results_summary.append({\"vector_column\": vector_column, **evaluation_results})\n",
        "\n",
        "    # Summary of evaluation scores\n",
        "    results_summary_df = pd.DataFrame(results_summary)\n",
        "    print(\"\\nSummary of evaluation scores:\")\n",
        "    print(results_summary_df)\n",
        "    if output_csv:\n",
        "        results_summary_df.to_csv(\"clustering_evaluation_summary.csv\", index=False)\n",
        "        print(\"Saved clustering evaluation summary to clustering_evaluation_summary.csv\")\n",
        "\n",
        "# Example usage\n",
        "# main(\"path_to_file.xlsx\", [\"bert-base-uncased\", \"paraphrase-multilingual-MiniLM-L12-v2\", \"All-MiniLM-L12-v2\", \"roberta-large-nli-stsb-mean-tokens\"], category_column=\"category\")\n",
        "\n",
        "\n",
        "\n",
        "main(chin, vector_columns, category_column=\"category\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "c5v-yPwaQNrB",
        "outputId": "9ed86b44-3472-4eb0-e207-09c570217c99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing for vector representation: Vector bert-base-uncased\n",
            "Evaluation results for Vector bert-base-uncased:\n",
            "Adjusted Rand Index: 0.1490\n",
            "Normalized Mutual Information: 0.3187\n",
            "Homogeneity: 0.3102\n",
            "Completeness: 0.3276\n",
            "V-Measure: 0.3187\n",
            "Fowlkes-Mallows Index: 0.2862\n",
            "Silhouette Score: 0.0814\n",
            "Davies-Bouldin Index: 2.7721\n",
            "Calinski-Harabasz Index: 5.0962\n",
            "Best Metric for Vector bert-base-uncased: Calinski-Harabasz Index with value 5.0962\n",
            "\n",
            "Processing for vector representation: Vector paraphrase-multilingual-MiniLM-L12-v2\n",
            "Evaluation results for Vector paraphrase-multilingual-MiniLM-L12-v2:\n",
            "Adjusted Rand Index: 0.1135\n",
            "Normalized Mutual Information: 0.2782\n",
            "Homogeneity: 0.2617\n",
            "Completeness: 0.2970\n",
            "V-Measure: 0.2782\n",
            "Fowlkes-Mallows Index: 0.2700\n",
            "Silhouette Score: 0.0901\n",
            "Davies-Bouldin Index: 2.8385\n",
            "Calinski-Harabasz Index: 4.6036\n",
            "Best Metric for Vector paraphrase-multilingual-MiniLM-L12-v2: Calinski-Harabasz Index with value 4.6036\n",
            "\n",
            "Processing for vector representation: Vector All-MiniLM-L12-v2\n",
            "Evaluation results for Vector All-MiniLM-L12-v2:\n",
            "Adjusted Rand Index: 0.0566\n",
            "Normalized Mutual Information: 0.2362\n",
            "Homogeneity: 0.2290\n",
            "Completeness: 0.2439\n",
            "V-Measure: 0.2362\n",
            "Fowlkes-Mallows Index: 0.2094\n",
            "Silhouette Score: 0.0440\n",
            "Davies-Bouldin Index: 3.3905\n",
            "Calinski-Harabasz Index: 2.7798\n",
            "Best Metric for Vector All-MiniLM-L12-v2: Davies-Bouldin Index with value 3.3905\n",
            "\n",
            "Processing for vector representation: Vector roberta-large-nli-stsb-mean-tokens\n",
            "Evaluation results for Vector roberta-large-nli-stsb-mean-tokens:\n",
            "Adjusted Rand Index: 0.0643\n",
            "Normalized Mutual Information: 0.2434\n",
            "Homogeneity: 0.2413\n",
            "Completeness: 0.2455\n",
            "V-Measure: 0.2434\n",
            "Fowlkes-Mallows Index: 0.2024\n",
            "Silhouette Score: 0.0546\n",
            "Davies-Bouldin Index: 2.8719\n",
            "Calinski-Harabasz Index: 3.8917\n",
            "Best Metric for Vector roberta-large-nli-stsb-mean-tokens: Calinski-Harabasz Index with value 3.8917\n",
            "\n",
            "\n",
            "Summary of evaluation scores:\n",
            "                                  vector_column  Adjusted Rand Index  \\\n",
            "0                      Vector bert-base-uncased             0.148958   \n",
            "1  Vector paraphrase-multilingual-MiniLM-L12-v2             0.113475   \n",
            "2                      Vector All-MiniLM-L12-v2             0.056563   \n",
            "3     Vector roberta-large-nli-stsb-mean-tokens             0.064270   \n",
            "\n",
            "   Normalized Mutual Information  Homogeneity  Completeness  V-Measure  \\\n",
            "0                       0.318675     0.310189      0.327637   0.318675   \n",
            "1                       0.278233     0.261719      0.296971   0.278233   \n",
            "2                       0.236202     0.228978      0.243896   0.236202   \n",
            "3                       0.243356     0.241263      0.245486   0.243356   \n",
            "\n",
            "   Fowlkes-Mallows Index  Silhouette Score  Davies-Bouldin Index  \\\n",
            "0               0.286151          0.081401              2.772065   \n",
            "1               0.270014          0.090070              2.838504   \n",
            "2               0.209368          0.043966              3.390459   \n",
            "3               0.202385          0.054558              2.871934   \n",
            "\n",
            "   Calinski-Harabasz Index  \n",
            "0                 5.096155  \n",
            "1                 4.603648  \n",
            "2                 2.779826  \n",
            "3                 3.891746  \n",
            "Saved clustering evaluation summary to clustering_evaluation_summary.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multy language clustering"
      ],
      "metadata": {
        "id": "vmpp2yZ_jG6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "def load_data(file_path):\n",
        "    \"\"\"Load data and handle encoding issues.\"\"\"\n",
        "    data = pd.read_excel(file_path, engine=\"openpyxl\")\n",
        "\n",
        "    # Decode text fields to UTF-8\n",
        "    for col in [\"Proverbs\", \"trimmed\"]:\n",
        "        if col in data.columns:\n",
        "            data[col] = data[col].astype(str).str.encode(\"utf-8\", errors=\"ignore\").str.decode(\"utf-8\")\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "# Convert vector column from string to numpy array\n",
        "def process_vectors(data, vector_column):\n",
        "    \"\"\"Convert vector column from string to numpy array.\"\"\"\n",
        "    data[vector_column] = data[vector_column].apply(lambda x: np.array(eval(x)))\n",
        "    return data\n",
        "\n",
        "\n",
        "# Perform clustering\n",
        "def perform_clustering(vectors, num_clusters):\n",
        "    \"\"\"Perform KMeans clustering.\"\"\"\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "    clusters = kmeans.fit_predict(vectors)\n",
        "    return clusters\n",
        "\n",
        "\n",
        "def filter_same_language_pairs(data, cluster_column, vector_column, threshold_min=0.75, threshold_max=0.91):\n",
        "    \"\"\"Filter pairs within clusters by similarity score.\"\"\"\n",
        "    filtered_results = []\n",
        "\n",
        "    for cluster_id in data[cluster_column].unique():\n",
        "        cluster_data = data[data[cluster_column] == cluster_id].reset_index(drop=True)  # Reset index\n",
        "        vectors = np.stack(cluster_data[vector_column].values)\n",
        "        similarities = cosine_similarity(vectors)\n",
        "\n",
        "        for i, row1 in cluster_data.iterrows():\n",
        "            for j, row2 in cluster_data.iterrows():\n",
        "                if i < j and row1[\"Language\"] != row2[\"Language\"]:\n",
        "                    similarity = similarities[i, j]\n",
        "                    if threshold_min <= similarity <= threshold_max:\n",
        "                        filtered_results.append({\n",
        "                            \"sentence_1\": row1[\"trimmed\"],\n",
        "                            \"language_1\": row1[\"Language\"],\n",
        "                            \"sentence_2\": row2[\"trimmed\"],\n",
        "                            \"language_2\": row2[\"Language\"],\n",
        "                            \"cosine_similarity\": similarity,\n",
        "                            \"cluster\": cluster_id\n",
        "                        })\n",
        "\n",
        "    return pd.DataFrame(filtered_results)\n",
        "\n",
        "\n",
        "# Main workflow for clustering all languages together\n",
        "def unified_clustering(files, vector_columns, num_clusters=20, similarity_threshold_min=0.75, similarity_threshold_max=0.91, output_csv=True):\n",
        "    \"\"\"Unified clustering workflow.\"\"\"\n",
        "    # Load and combine all datasets\n",
        "    combined_data = []\n",
        "    for lang, file in files.items():\n",
        "        data = load_data(file)\n",
        "        data[\"Language\"] = lang  # Add language identifier\n",
        "        combined_data.append(data)\n",
        "\n",
        "    combined_data = pd.concat(combined_data, ignore_index=True)\n",
        "\n",
        "    for vector_column in vector_columns:\n",
        "        print(f\"Processing for vector representation: {vector_column}\")\n",
        "\n",
        "        # Process vectors\n",
        "        combined_data = process_vectors(combined_data, vector_column)\n",
        "\n",
        "        # Stack all vectors for clustering\n",
        "        all_vectors = np.stack(combined_data[vector_column].values)\n",
        "\n",
        "        # Perform clustering\n",
        "        clusters = perform_clustering(all_vectors, num_clusters)\n",
        "        combined_data[f\"{vector_column}_cluster\"] = clusters\n",
        "\n",
        "        print(f\"Clustering completed for {vector_column}.\")\n",
        "\n",
        "        # Filter cross-language pairs within clusters by similarity\n",
        "        cross_language_pairs = filter_same_language_pairs(\n",
        "            combined_data,\n",
        "            cluster_column=f\"{vector_column}_cluster\",\n",
        "            vector_column=vector_column,\n",
        "            threshold_min=similarity_threshold_min,\n",
        "            threshold_max=similarity_threshold_max,\n",
        "        )\n",
        "\n",
        "        # Save the results\n",
        "        if output_csv:\n",
        "\n",
        "            filtered_output_file = f\"filtered_cross_language_pairs_{vector_column.replace(' ', '_')}.csv\"\n",
        "            cross_language_pairs.to_csv(filtered_output_file, index=False)\n",
        "            print(f\"Saved filtered cross-language pairs to {filtered_output_file}\")\n",
        "\n",
        "\n",
        "# Define file paths and vector columns\n",
        "files = {\n",
        "    \"English\": \"updated_4437_English_proverbs-1 With vectors.xlsx\",\n",
        "    \"Hebrew\": \"updated_1867_Hebrew_Proverbs with Vectors.xlsx\",\n",
        "    \"Arabic\": \"updated_kaggle_Arabic_proverbs with Vectors.xlsx\",\n",
        "    \"French\": \"updated_kaggle_French_proverbs with Vectors.xlsx\",\n",
        "    \"Chinese\": \"updated_kaggle_Chinese_proverbs with Vectors.xlsx\"\n",
        "}\n",
        "\n",
        "vector_columns = [\n",
        "    \"Vector bert-base-uncased\",\n",
        "    \"Vector paraphrase-multilingual-MiniLM-L12-v2\",\n",
        "    \"Vector All-MiniLM-L12-v2\",\n",
        "    \"Vector roberta-large-nli-stsb-mean-tokens\"\n",
        "]\n",
        "\n",
        "# Run the unified clustering workflow\n",
        "unified_clustering(files, vector_columns)"
      ],
      "metadata": {
        "id": "TmZaNXsGakBq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fa28379-1d8d-4e04-cc27-345fa0415194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing for vector representation: Vector bert-base-uncased\n",
            "Clustering completed for Vector bert-base-uncased.\n",
            "Saved filtered cross-language pairs to filtered_cross_language_pairs_Vector_bert-base-uncased.csv\n",
            "Processing for vector representation: Vector paraphrase-multilingual-MiniLM-L12-v2\n",
            "Clustering completed for Vector paraphrase-multilingual-MiniLM-L12-v2.\n",
            "Saved filtered cross-language pairs to filtered_cross_language_pairs_Vector_paraphrase-multilingual-MiniLM-L12-v2.csv\n",
            "Processing for vector representation: Vector All-MiniLM-L12-v2\n",
            "Clustering completed for Vector All-MiniLM-L12-v2.\n",
            "Saved filtered cross-language pairs to filtered_cross_language_pairs_Vector_All-MiniLM-L12-v2.csv\n",
            "Processing for vector representation: Vector roberta-large-nli-stsb-mean-tokens\n",
            "Clustering completed for Vector roberta-large-nli-stsb-mean-tokens.\n",
            "Saved filtered cross-language pairs to filtered_cross_language_pairs_Vector_roberta-large-nli-stsb-mean-tokens.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def rewrite_file_for_download(input_file, output_file, file_format=\"csv\", encoding=\"utf-8\"):\n",
        "    \"\"\"\n",
        "    Reads a file and rewrites it with proper encoding for local download.\n",
        "\n",
        "    Parameters:\n",
        "    - input_file (str): Path to the input file.\n",
        "    - output_file (str): Path to save the rewritten file.\n",
        "    - file_format (str): Format to save the file ('csv' or 'excel').\n",
        "    - encoding (str): Encoding to use when saving the file (default: 'utf-8').\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read the input file (CSV or Excel)\n",
        "        if input_file.endswith(\".csv\"):\n",
        "            data = pd.read_csv(input_file, encoding=\"utf-8\")\n",
        "        elif input_file.endswith(\".xlsx\"):\n",
        "            data = pd.read_excel(input_file, engine=\"openpyxl\")\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file format. Use CSV or Excel.\")\n",
        "\n",
        "        # Rewriting the file\n",
        "        if file_format == \"csv\":\n",
        "            data.to_csv(output_file, index=False, encoding=encoding)\n",
        "            print(f\"File successfully rewritten as CSV: {output_file}\")\n",
        "        elif file_format == \"excel\":\n",
        "            data.to_excel(output_file, index=False, engine=\"openpyxl\")\n",
        "            print(f\"File successfully rewritten as Excel: {output_file}\")\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported output format. Use 'csv' or 'excel'.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file: {e}\")\n",
        "\n",
        "\n",
        "input_file = \"filtered_cross_language_pairs_Vector_bert-base-uncased.csv\"\n",
        "output_file_csv = \"filtered_cross_language_pairs_cleaned.csv\"\n",
        "output_file_excel = \"filtered_cross_language_pairs_cleaned.xlsx\"\n",
        "\n",
        "# Rewrite the file as CSV\n",
        "rewrite_file_for_download(input_file, output_file_csv, file_format=\"csv\", encoding=\"utf-8\")\n",
        "\n",
        "# Rewrite the file as Excel\n",
        "rewrite_file_for_download(input_file, output_file_excel, file_format=\"excel\")\n"
      ],
      "metadata": {
        "id": "SNcZeYNjwNnO",
        "outputId": "49bfed12-2159-4f7d-9720-d30b9f3523d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File successfully rewritten as CSV: filtered_cross_language_pairs_cleaned.csv\n",
            "File successfully rewritten as Excel: filtered_cross_language_pairs_cleaned.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking to see how many rows for each of the embedding models got assigend in the multy language phase - for the results."
      ],
      "metadata": {
        "id": "PGn8glThiXFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def check_file_rows_with_filter(prefix=\"filter\"):\n",
        "    \"\"\"\n",
        "    Checks the number of rows in all files starting with a specific prefix in the current directory.\n",
        "\n",
        "    Parameters:\n",
        "    - prefix (str): The prefix to filter files (default: \"filter\").\n",
        "    \"\"\"\n",
        "    # Get all files in the current directory\n",
        "    all_files = [f for f in os.listdir() if f.startswith(prefix)]\n",
        "\n",
        "    for file in all_files:\n",
        "        try:\n",
        "            # Determine file type and load accordingly\n",
        "            if file.endswith(\".csv\"):\n",
        "                data = pd.read_csv(file, encoding=\"utf-8\")\n",
        "            elif file.endswith(\".xlsx\"):\n",
        "                data = pd.read_excel(file, engine=\"openpyxl\")\n",
        "            else:\n",
        "                print(f\"Unsupported file format: {file}\")\n",
        "                continue\n",
        "\n",
        "            # Count rows\n",
        "            num_rows = len(data)\n",
        "            print(f\"File: {file} | Rows: {num_rows}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading file {file}: {e}\")\n",
        "\n",
        "\n",
        "# Run the row check for all files starting with \"filter\"\n",
        "check_file_rows_with_filter(prefix=\"filter\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scuxrCw5RgFm",
        "outputId": "52e00b94-3fdc-419f-ef3a-e8a1ecddbd5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File: filtered_cross_language_pairs_Vector_bert-base-uncased.csv | Rows: 12082\n",
            "File: filtered_cross_language_pairs_Vector_roberta-large-nli-stsb-mean-tokens.csv | Rows: 887\n",
            "File: filtered_cross_language_pairs_Vector_paraphrase-multilingual-MiniLM-L12-v2.csv | Rows: 22098\n",
            "File: filtered_cross_language_pairs_Vector_All-MiniLM-L12-v2.csv | Rows: 61\n"
          ]
        }
      ]
    }
  ]
}